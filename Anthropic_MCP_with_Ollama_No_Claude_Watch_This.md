---
youtube_url: https://www.youtube.com/watch?v=9mciRwpcLNY
title: Anthropic MCP with Ollama, No Claude? Watch This!
tags: []
description: "anthropic released model context protocol which allows you to connect llm's to your own data and tools.  in this video chris shows how to decouple mcp from claude desktop and claude 3.5 sonnet, and run mcp with your own applications natively, and use open source llm's such as qwen 2.5, llama 3.2, ibm granite via ollama or even use gpt-4o or gpt-4o-mini from openai\n\ngithub repo: \nhttps://github.com/chrishayuk/mcp-cli"
uploader_id: '@chrishayuk'
channel: Chris Hay
upload_date: '20241203'
duration: 00:29:54
audio_quality: medium
---
00:00:00 - 00:05:00

Hey, welcome back. So about a week ago, Anthropic Open Source Model Context Protocol, which is their new way of connecting LLMs to external data sources and tools, and we as a community went absolutely wild about it. But one of the problems that we've had is that it can only talk to Cloud Sonnet model, and you can only really access it through Cloud Desktop. We haven't got a way of easily connecting our own LLMs, especially ones on Ollama or OpenAI, and we really wanna be able to use Model Context Protocol within our own applications, and that has not been easy to do. So today, I'm gonna show you how you can build your own native Model Context Protocol application, which connects to LLMs hosted on Ollama and on OpenAI. So just as a reminder, in my previous video, I showed you how you could get started with Model Context Protocol on Cloud Desktop and connect to a SQLite database, which has a products table. What we really want to be able to do is use Model Context Protocol with our own applications. And we also want to be able to use different large language models. We don't always want wanna use CloudSonic. So I'm gonna do the exact same demo, but this time I'm gonna do it within my own native CLI. So if we just run that for a second, and you see in my CLI, I've got a bunch of commands that allows me to talk to the Model Context Protocol servers. I'm gonna type in chat, and that opens up an interactive chat mode. And as you can see, I've entered the chat mode using provider OpenAI and GPT-4.0 mini. So I'm not even using GPT-4.0. I'm going to use mini here. And then I am just going to say what tables are available. And shocks above, it is now making a tool call. We see tool list tables. So exactly what Cloud Desktop did. And it's came out with the three tables products users and sqlite sequence so i will say describe products and again once again you can see a tool command describe table invoke with arguments table name products and then it tells me the schema and then i'm just going to say select top 10 products ordered by price then as you see here it's doing another tool invoke there so it's saying a tool read query invoke with arguments query select star from products ordered by price limit 10 and then it's came back with screen protector mouse pad etc. The key thing is here I'm talking to the same model context provider as I did before but instead this is running in my own CLI there is no cloud desktop and more importantly I'm actually using GPT-4.0 mini rather than cloud 3.0 Sonnet. But I'm not restricted to GPT-4.0 mini. If I want to, I can use Lama and actually talk to any of my open source models. So to do that, I really need to pick a model that is able to talk function call. And I'll go through that in a second. So I'm going to pick, this case Lama 3.2, but equally works well with things like Quan. It works with things like the IBM's Granite model. It works with any model that is good at function calling. So in this case, I'm going to pick Lama 3.2, and then I am just going to do the exact same demo. So I'm going to go into chat mode, and I'm going to say what tables are available. And then again, as you can see, it's calling list tables. It's calling describe table. And then it's coming back with the columns. And then I'm just going to say select top 10 products ordered by price in descending order. And there you go. It's working with all Llama and Llama 3.2 models. So as I said before, I am using my repo here, which is on github.com, crusadeuk, ncp-cli. And then if you want to get that running with OpenAI, then all you really need to do is in your.env file, set OpenAI on the score API key to whatever your exact key is. If, of course, you want that to work with Ollama, then you won't need to do that. The key thing is you are just going to run uv run. Again, if you want to use OpenAI, you don't need to set a provider or a model. You can just run it like this, and then that will, of course, work. Or if you are wanting to use Ollama directly, then you just need to set the provider, which would be Ollama and at Llama 3.2. But what is cool about this CLI is, and the way that I've built this, is that actually I've gone down to the protocol level. So I'm going to break down exactly how this works. I'm going to show you a little bit about how the CLI works so that you can go and be able to create your own applications. So the first thing I want to do is go through the architecture a little bit and then I'll look at the CLI and then I'll show you how we get started in this. So in the demo that I showed you earlier, and again I covered this in my other video, but we have this idea of a host. So in this case MCP CLI or the cloud desktop application was effectively the host. And then within that host, we can run a client application. So client three, client one, et cetera. And then on the right-hand side here is we have our servers. So when that SQLite server database that we're talking to,

00:05:00 - 00:10:00

that was actually an MCP server. So it was a SQLite server. And then that's talking to the resources. and then the host and their clients are effectively talking to these servers that's that's really what's going on underneath the hood so in order for that to work those servers so in this case the server that I'm running is actually a standard IO server so if we look at that for a second, so if you look in my server config here, you're going to see that I've got MCP servers and I've got a server called SQLite. And really what I'm doing is running UVX and then I'm calling MCP server SQLite and then I'm passing in test DB as the path. So in my folder I have a testdb within there you can see it there and if I was to go back into my terminal and again I covered this in my other video but if I was to type in something like sqlite3 and then I typed in test.db I would be able to go select star from products and then you would and then you would be able to go select star from products. And then you would, and then you would be able to see the items that were in the database, because that's what's actually happening, because that's actually what's happening on the hood. So this sort of database here, so the testdb database is essentially acting as the database on this diagram and MCP server SQLite is acting as the server on the right hand side. So if I do a uv run main pi dash dash server SQLite for a second it's going to connect to that configuration if I wanted to change the configuration I just change it my server config but as I said there's a set of commands here so you see ping checks if the server is responsive so that's probably the first thing about the protocol is that when a client is connecting to a server there is a message it needs to send which is an initialized message which will get its capabilities and then if I want to check if a server is available I can send it a ping command and then it will basically respond if it's there. So you see I'm pinging server and the server is up and running. So if I were to go into my main.py for a second you can kind of see the handle command here and then it says if command equals ping, pinging server we saw that and then I'm gonna send a message called send ping and then you can see read stream and write stream. So I've passed in as parameters read stream and write stream so I'm gonna send a ping and in this case it's gonna be the standard IO read and write strings there so ping. And in this case, it's going to be the standard IO read and write strings there. So it's my input and output. And it's going to send the ping message. You can see send ping is coming from messages.ping. I'm going to show you what that looks like in a second. You can also see the send initialized message. We'll go through that in a second. And then if I go down a little bit further in my main, just before I call that handle command there, you see the first thing that happens is I'm going to load my server configuration config. So await load config. And then I'm going to establish standard IO communication. You see this async with standard IO client. I'm passing in my server parameters, i.e. the name of the client I'm going to correct, i.e. the name of the server that I'm going to connect to. I'm going to create a read stream and a write stream. And then the first thing I do is initialize the server, i.e. get the capabilities, and then it's going to handle that command there. So that is really what's happening. And then I'm just running around in a loop to try and get kind of more messages. So and again we can see this so if I switch out of critical mode here for a second and I switch to debug mode and then we just do this one more time. So now that I've got debug mode open you can see here the first thing that happens I'm loading config from server config which I just showed you my JSON file and then there's that command uvx and it's running MCP server SQLite and it's passing in testdb as the db path. And you see here subprocess started with PID 47095. So if I actually just run here for a second, do ps minus a, and there's 47095, and you can see opt homebrew uv, toolX, MCP Server SQLite, and the path is dbtest there. So you can see in this particular case, standard I.O. is effectively running a process. It's a sub-process in my machine. It's running in the background. And then effectively, I'm just using I.O. to send commands back and forward. Now, MCP does support HTTP servers as well And in particular uses server-side events to be able to do that and maybe cover that in another video Maybe cover this in another version, but effectively you can kind of see that that is what's happening It's just a process that's running on my machine And then I'm just sending essentially standard in standard out to be able to communicate with that. So first thing that happens there the next thing that you can kind of see here is that uh i'm going to send this uh piece of json this is the initialized command see this j and it's basically it's a thing called json rpc so you see it's json rpc 2.0 i'm sending an id, in this case, init1. The method, that's the important part for the

00:10:06 - 00:15:06

messages, is the initialized message. And then I'm telling it what I support. So I'm not going to go through the details of this, but I'm basically saying I've got protocol version. And then I tell you what capabilities my client is capable of supporting. In this case, routes, list change, sampling, et cetera. And I give it the name of my client and my version as well it's not important at the moment maybe in a deeper deeper dive we maybe go through that and then of course once i've sent that data um it's going to go across to the server and then it's going to respond back to me so you see i get a message back saying json rpc for init1 so it's acknowledged my message uh and then it tells you what capabilities it has as well so it's saying okay it's experimental it's list changed etc and it gives all the details that it support but that initialization is really the first message that happens and then finally I'm just going to send a notification message so it's told me I've told it my capabilities it's told me its capabilities and then I'm just sending a notification to me, I've told it my capabilities, it's told me its capabilities, and then I'm just sending a notification to say, okay, I've initialized my client. And then if I want to quit out the client, I don't need to send it any other messages, I can just close out there. And again, if I come back into the, and again, if I come back into MCP specification for a second, so just to sort of explain what we've seen here, if I click on this kind of lifecycle piece of the specification, you see it talks about the lifecycle of client-server connections. So initialization is the thing that we just did. We sent what our capabilities back and forward. So you see here initialized request, and then I got that response. And then I sent back a notification to the server to say that I was initialized. That was effectively what we just saw there and then the next part is from an operation phase I'm going to send it some protocols and then if I want to disconnect I can just close the connection and then we're done. So normal protocol operations is things like pings, etc. But before I get on to that, just to sort of go even further here, there is the spec, JSONRPC 2.0 ID1 method initialized, parameters, capabilities, sample, and client info. And then the server's got to respond with its own capabilities information, which it just did. And of course, as I showed you there, this is my initialized message. And then this is the capability sent back by the server, and then eventually I send it a notification. So I'm literally walking through this spec here on how that works. And then once that's done, I can send my own operations there. Now if you're thinking that's a little bit complicated, I've done a super simplified version here stripped it down it's a file called test.py and you can see I've got my messages send an initialized message ping etc. I've done this stripped down main method here where I just pass in config, pass in the server name which in this case is SQLite I call the load config and then here's that async with a standard io client passing in those parameters that I've just loaded from the config restream right stream and I initialize the server by sending that send initialize command we'll go through that in a second and then we check and if it works then we're connected so if I close this down for a second we'll just clear this if I do a ps minusa you'll see that 47095 has now disappeared so we're no longer running that sub process and now if I do a uv run and this time rather than calling main.py if I call test.py you can see same sort of thing and then it's sent, it's initialized, it's received its message, and then it's sent its initialized notification. And then I've just closed the stream because I was done. I didn't have anything else to do there. So if I do a PS minus A, you can see it's no longer running. We're good. And then, of course, if I wanted to extend this, if I wanted to talk to the server a little bit more, then I could just simply await a send ping there. So if I save this now and then we run mytest.py one more time, you can see now it's done everything that it did before, but now I'm sending a new message and that new message that I'm sending is the ping which is JSON RPC 2.0 the ID is ping1 and the method is ping so it's a very simple as format there and then the server is gonna receive it it's gonna do whatever it's gonna do and then it's gonna come back to me and send me the response of ping so ping1 method none parameters none and then you know is success so that sort of leads me on to what these messages look like so if i come into my messages folder for a second you can see i've got this sort of base jsonrpc message uh it's a pydantic class that i've created in this case but the key thing is you see jsonrpc 2.0 and then i'm accepting in an id method parameters result is error so you can just use this as a base message type. And then if we look at this, the ping message is super, super simple.

00:15:09 - 00:20:09

Because to send the ping, all I am going to do is I'm going to call a thing called sendMessage, pass the read stream, the write stream, the method is ping, ping1. And then I'm going to get that answer. So it's really simple. I'm just passing in a message ID, and then I'm calling the sendMessage. You can imagine what sendMessage is going to do. It is taking all of this, it's going to create that JSONRPC message that we had a second ago. I'm going to pass in my message ID, my method, and my parameters. And then I've got a little bit of sort of retry logic in here where I'm going to attempt to send the message. Sends it onto that write stream, so it's just doing a standard IO right off it goes And then if it fails then I'll get a timeout whatever and then once it's done We're good to go now if I look at send initialize message It's a bit more complicated because I've got all of these sort of client capabilities initialize parameters Blah blah blah, and then I'm gonna I'm gonna initialize them set my protocol version but eventually I'm gonna send that message put those parameters in as JSON and it's just initialized and then once I do that send I'm gonna expect to get a result and then I'm gonna once I've got that result I will then eventually send that notification to say it's done. The key thing I want you to notice when I'm chatting with this is I'm not actually using any of the libraries from Anthropic on the Model Context Protocol website. I've literally implemented this from scratch by hand just using JSON RPC. And then that shows you that it's pretty easy for you to be able to do that yourself. Now, if I come back into the CLI for a second, I'm just going to set my logging back to critical because we don't want to look at the debug. And I'm gonna rerun my main.py, so back into my CLI. We'll click Help again there. You can see that there's a few things that I've got available. So ping checks if the server is responsive. So we've done that one already. So ping, server's up and running. I can clear it. But there's a few other commands that we've got here we've got things like list prompts now that is for um if you've got something like vs code or something or slack or something and you want to be able to do the app thing and see what prompts it supports you can you can have a look at the prompt list now in the sqlite server that i downloaded there which is one of the sort of default anthropic provided ones it it doesn't give you a lot of prompts here it just says a prompt to see the database for the initial data you know so it's not kind of much useful there it's very generic again if i do list resources resources are meant to be things like kind of file system basing. So it could be files, it could be content from a content management server. Again, because this is a SQL server database, it's an example project, it doesn't do anything useful. It's just coming back with memo insights. But you can imagine files and etc. So I'm just going to clear that. But the thing that you're probably most interested in is if I do something like a list tools, and you can see from the tools list this is the same tools queries that we were executing before so things like read query you can see things like create table for example describe table so if you remember the demo that we did with cloud desktop or within the CLI itself what was actually happening underneath the hood is when I said what tables are available in my database, you should see it's view result from list tables. Get a schema, view result from described table, and then select top 10 products, read query. There is a one-to-one mapping between here, read query, blah, blah, blah. So hopefully anybody who's ever done agents before is going to start to understand what's actually happening underneath the hood here is effectively what I'm doing is making a call to get a list of the tools that the model is able to interact with and those tools are going to be placed in the context so basically what you're seeing here this description is going to end up in the LLM's context and once it's in the LLM's context then the LLM is going to know what tools it's available to be able to work with. And then of course when it's making a question like, list me all the tables, for example, in my database, then it knows the tools that it's got and it's going to make those particular tool calls and the way that it's going to do that is via function calling. Because actually underneath the hood here, I'm giving it all the information that it needs to be able to make a function call. So you can kind of see here for the tool name, description, input schema. So it's type, object, properties, query, type, string, description, required. It is basically describing everything it needs to know to be able to do a function call. So how did I actually implement this in chat mode? Well, if we come into here for a second, so as you can see, when I open up chat, what's actually happening here is I call a handle called handle chat mode. So if I go into what my chat handler is for a second, this is kind of where the magic happens, and you're going to see exactly what's going on. So the very first thing that I do is, as you see, I call this fetch tools method.

00:20:13 - 00:25:13

And you can guess what fetch tools does. It is in my tools handler. And within my tools handler, you can see what I'm doing is calling send tools list. So I'm actually just making that exact same list tools call that we had. And then I'm going to get all the tools coming back. And then I'm going to stick it here. Now, the key thing that I'm going to do once I've got the tools list, you can see I've got this thing here called generate system prompt, where I'm taking the tools that have been returned back, and I'm going to generate a system prompt with that. I'm putting it in the context. So if I go to generate system prompt here, you can see this one in particular is going to call a thing called the system prompt generator and then I'm going to generate a prompt and I'm going to add it to the system prompt. So if I go to this generate prompt for a second, anyone who's ever worked with Claude before and looked at their kind of website before you are going to see this is going to feel very very familiar because you can see in this template in this environment you have access to a set of tools you can use to answer the question format instructions string and scalar parameters should be specified blah blah blah tool definitions in JSON schema, user system prompt tool configuration. And then I'm literally just going to generate a system prompt here. And then I'm also going to be adding on some general guidelines, step-by-step reasoning, analyze that systematically, blah, blah, blah, blah. And then this big description is effectively going to be what the LLM is dealing with. So once I enter chat mode I have got a very large system prompt that tells you exactly what Tools the models got to deal with and how I want it to be able to you do tool usage So everything from step-by-step reasoning blah blah blah clear communication and give it examples of how it works So it is stuffed into that system prompt. And that's exactly what happens when you're working with agents, right? You fill the agent with the context so it knows exactly how to be able to call the tools and what tools it is available to. So if I come back into my chat handler for a second, I've now generated my system prompt. The next thing I need to be able to do is be able to do function calling. So what I do here is I convert the tools into OpenAI format. So if you call here, I just take the format that we've got. So I've made that call to the database schema. And then all I'm going to do is turn that into OpenAI format so that function calling will work. And then I'm just going to initialize my LLM client here. I've got a nice little LLM client thing where I abstract away a few things. I could use something like LLM, but I wanted just to hook something up really quickly. So in this case, you see here, I've set my provider to OpenAI and the default model's GPT-40-mini. And then in here, it's just standard native OpenAI and it's standard native Ollama so this case this LLM client here I usually have got create completion call OpenAI completion if the provider's Ollama then it'll call the Ollama completion and then it's just going to do the normal chat completions key thing that is happening here look at the OpenAI version. The tools that I've gotten back from my tool call via the MCP server, I'm passing in as tools in my chat completions natively. That is effectively function calling. It is just calling tools. I'm just using tools within function calls. That's it. And then I'm going to get the response back and return it. So you see here tools calls, and then I'm'm just gonna get the result of the tool call there. Probably same thing for Ollama completion, similar thing and in this case I default to Kanaquen but as I say I pass through a model in this case Lama 3.2. That worked, I get the response and then I'm gonna fill in that tools call. So the format that I get is slightly different from Ollama so I just need to convert it to a sort of generic way of doing that and then that's good. So if I come back into my chat handler I set up my conversation history so you know so so far I've set it up I've set up what my tool calls are I've set up my conversation history I hit it chat mode and then I'm just going to chat as normal. I'm going to add to the conversation history and then when I do process conversation when I call the LLM I call that completion. I'm going to get my tools back but the key thing is if I have a tool call coming back from the LLM I am then going to handle my tool call. So I'm going to call handle tool call and then you can imagine what's going to happen here. A little bit complicated but the key thing that happens here is I'm going to call handle tool call. And then you can imagine what's going to happen here. A little bit complicated, but the key thing that happens here is I'm going to pass, parse that tool call message. I'm going to get the name of the tool, the function name, the arguments, because the LLMs worked all of that out. It's giving me back, you need to call this function. These are the arguments you need to pass. And then eventually, once I've parsed all of that, I'm eventually going to have to call the tool. And what am I doing here? I'm just doing another message. And this time is called send call tool. So if I go to definition on that,

00:25:14 - 00:30:14

it's just like any other JSON RPC message. I'm passing through send message. Here's my restream. Here's my right stream. The method in this case is tools forward slash call. I pass in the name of the tool, and then I pass in the argument. So in this case it would be list tables, maybe no arguments, or it would be a re-query and then it pass in all the arguments that I want to run, i.e. the SQL statement is going to perform that and it's just another JSON or PC message. And eventually once I get the response back then I'm just going to add the tool call into the conversation history. I'm going to add all the response in there and then I'm just gonna add the tool call into the conversation history I'm gonna add all the response in there and then I'm gonna continue on so to bring it back into this if you are just curious and again I sort of said this before you know this is my sequel like database this is my sequel light server which was the default quick start in fact that quick start is available here so if you go in the Model Context Protocol IO Quickstart, you see this is the database that you create. These are the exact same products that we vented in there. I have a video on how to do this. And then you can call MCP Server SQLite, which is downloaded from the sort of sample servers that are available, my model context protocol. So I didn't create MCP server SQLite, it was just available. But you can see with that running as a servers, running called desktop talk to it, using the code that I created by recreating the protocol, I'm able just to interact with this. And at the end of the day, though, all that has happened, so MCP server a here is that sql uh light server that we showed you there this is the sqlite database and then the protocol in this case is standard io but of course it could be uh http with server-side events i run that call that send initialize i get the capabilities once i've got the capabilities the next thing i'm going to do is i'm going to operate the command in chat mode. In this case, I'm going to list the tools, so I get the tools back. And then all I'm going to do is when the LLM decides that it needs to make a tool call in the same way as agents do, i.e. via function calls, then it's just going to make a send call back to that server there. That is it. That is exactly how it works. This is the diagram that sort of shows the same thing there. Cloud desktop, MCP server, SQLite database, initialize available quill abilities, query request, perform the SQL query, get the result, and then have the formatted results. The only difference in this diagram is I'm not using Cloud desktop. I'm using my MCP CLI. Everything else is the same. And then from an LLM point of view, rather than using Sonnet, I'm using my own LLM. And then just to bring this home one more time, so if I do call main.py, pass in a SQL Server Lite, providers are Lama and Lama 3.2. The reason I picked Lama 3.2 is because that's the first version of Lama that supports function calling. Go into chat mode, and then back to what I said before, I can just say what tables are available. And you're gonna see, it's gonna make that, it's gonna do a first call, which is list tables to figure out what tables there, tell me it's products uses SQLite sequence, and then I'll say what columns are in products. And this part here is the LLM interpreting that, but then because the tools are in the context and because the LLM knows how to handle that and make a function call, it's then going to recommend that you use the describe table. Once it gets the describe table coming back from the tool response and the function call, then I can make the send call message to go and get ID, name, and price. And then the LLM takes all that back and then returns the response. And then I can say select top 10 products by price. And the same thing happens over and over again. This is just function calling. The only difference is I'm using send call and doing function calling via the MCP server as opposed to having the tool locally on my machine. And there's a really nice protocol that defines the difference. So there you go. You should understand how MCP works now. More importantly, you can go and build your own application, right? Use MyGitHub as an example, but you can go and build your own application. You can go and build your own application, right? Use MyGitHub as an example. But you can go and build your own application. You can go and build your own servers. And then you can use your own LLMs. So you can go use Ollama. You can go and use Cloud. You can use whatever you want. You're not held to using Cloud 3.5 on it. Anyway, I hope this video has been useful and you've had a bit of a deep dive into MCP and hopefully you'll go off and create your own awesome applications and use your own models to go and talk to MCP and grow the ecosystem. And I'll catch you on the next one.

